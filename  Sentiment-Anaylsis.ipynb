{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Homework 3\n",
    "## Sentiment analysis using Neural Networks\n",
    "\n",
    "Total: 50 Points\n",
    "\n",
    "\n",
    "In this homework we will perform sentiment analysis using a few simple Neural Network based architectures.\n",
    "For this problem we use the IMDB Large Movie Review Dataset. The dataset contains 25,000 highly polar movie reviews for both train and test dataset, each with 12,500 positive (greater than equal to 7/10 rating) and 12,500 negative reviews(less than equal to 4/10 rating). \n",
    "\n",
    "Use \"https://keras.io/\" for keras documentation. Please use Python 3. GPU is not required but it will help improve the training speed for each problem.\n",
    "\n",
    "Please save the notebook with your cell outputs. You will not be graded if your outputs are not present below the homework cell. Also note your outputs will be unique since you will be using your the last numbers of your uni as your random seed (In the third cell). Make sure you submit this iPython file, with the saved outputs. The submission format must be 'hw3/hw3.ipynb'. You will not submit any other files. If you do save your model weights, you will not submit them. You will however, make sure your model weights do get saved in the 'weights' folder and can be retrieved from there as well.\n",
    "\n",
    "Please fill your details below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Name: Haiqi Zhu\n",
    "\n",
    "Uni: hz2432\n",
    "\n",
    "Email: hz2432@columbia.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D, GRU, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "#from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#we retrieve train and test file names\n",
    "\n",
    "train_dir = \"./aclImdb/train/\"\n",
    "test_dir = \"./aclImdb/test/\"\n",
    "tr_review = [re_filename for re_filename in listdir(train_dir)]\n",
    "te_review = [re_filename for re_filename in listdir(test_dir)]\n",
    "\n",
    "#we initialize the train and test arrays\n",
    "\n",
    "tr_X = []\n",
    "tr_Y = []\n",
    "te_X = []\n",
    "te_Y = []\n",
    "\n",
    "#we arrange the reviews into the train and test arrays \n",
    "\n",
    "for review_file in tr_review:\n",
    "    f_review = open(train_dir+review_file, \"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    tr_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        tr_Y.append(1)\n",
    "    else:\n",
    "        tr_Y.append(0)\n",
    "        \n",
    "for review_file in te_review:\n",
    "    f_review = open(test_dir+review_file, \"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    te_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        te_Y.append(1)\n",
    "    else:\n",
    "        te_Y.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the validation set from the train set\n",
    "\n",
    "use the last 4 numbers of your uni for the seed value seed to ensure all answers remain unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2547\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#replace 2 (SEED) with the last 4 numbers of your Uni\n",
    "#Uni: hz2432\n",
    "SEED = 2432\n",
    "seed_counter = 0\n",
    "while(1):\n",
    "\n",
    "    shuffle_combine = list(zip(tr_X, tr_Y))\n",
    "    random.seed(SEED+seed_counter)\n",
    "    seed_counter+=1\n",
    "    random.shuffle(shuffle_combine)\n",
    "\n",
    "    tr_X, tr_Y = zip(*shuffle_combine)\n",
    "\n",
    "    val_X = tr_X[:5000]\n",
    "    val_Y = tr_Y[:5000]\n",
    "\n",
    "    counter = 0\n",
    "    for label in val_Y:\n",
    "        counter+=label\n",
    "\n",
    "    print (counter)\n",
    "    print (seed_counter)\n",
    "    if(counter>2400 and counter <2600):\n",
    "        tr_X = tr_X[5000:]\n",
    "        tr_Y = tr_Y[5000:]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train review set : 20000\n",
      "Length of Train label set : 20000\n",
      "Length of Validation review set : 5000\n",
      "Length of Validation label set : 5000\n",
      "Length of Test review set : 25000\n",
      "Length of Test label set : 25000\n",
      "*****************************************\n",
      "Some sample Reviews Train sets and their labels\n",
      "I've recently watched this movie, in a lazy Sunday afternoon, with some friend of mine and we have a lot of fun! This movie is a masterpiece of trash.\n",
      "1\n",
      "Repugnant Bronson thriller. Unfortunately, it's technically good and I gave it 4/10, but it's so utterly vile that it would be inconceivable to call i\n",
      "0\n",
      "This film has so little class in comparison to Strangers on a Train or even, Accidental Meeting for that matter, that despite plot similarities I woul\n",
      "0\n",
      "\"Jared Diamond made a point in the first episode that other peoples of the world didn't have animals to domesticate but Europeans did, and that accoun\n",
      "1\n",
      "By 1945, and after a string of solid WWII propaganda pieces, Errol Flynnâ€™s hold over U.S. box office had started to decline so, in spite of the increa\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Length of Train review set : \" + str(len(tr_X)))\n",
    "print(\"Length of Train label set : \" + str(len(tr_Y)))\n",
    "print(\"Length of Validation review set : \" + str(len(val_X)))\n",
    "print(\"Length of Validation label set : \" + str(len(val_Y)))\n",
    "print(\"Length of Test review set : \" + str(len(te_X)))\n",
    "print(\"Length of Test label set : \" + str(len(te_Y)))\n",
    "print(\"*****************************************\")\n",
    "print(\"Some sample Reviews Train sets and their labels\")\n",
    "print(tr_X[0][:150])\n",
    "print(tr_Y[0])\n",
    "print(tr_X[1][:150])\n",
    "print(tr_Y[1])\n",
    "print(tr_X[2][:150])\n",
    "print(tr_Y[2])\n",
    "print(tr_X[3][:150])\n",
    "print(tr_Y[3])\n",
    "print(tr_X[4][:150])\n",
    "print(tr_Y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we collect all the reviews from train validation and test set to generate \n",
    "texts = []\n",
    "texts += tr_X \n",
    "texts += te_X \n",
    "texts += val_X\n",
    "len(texts)\n",
    "\n",
    "\n",
    "\n",
    "#we clip the sentence length to first 250 words. \n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "#length of vocab, Tokenizer will only use vocab_len most common words\n",
    "vocab_len = 25000\n",
    "\n",
    "#we tokenize the texts and convert all the words to tokens\n",
    "tokenizer = Tokenizer(num_words=vocab_len)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "token_tr_X = tokenizer.texts_to_sequences(tr_X)\n",
    "token_te_X = tokenizer.texts_to_sequences(te_X)\n",
    "token_val_X = tokenizer.texts_to_sequences(val_X)\n",
    "\n",
    "#to ensure all reviews have the same length, we pad the smaller reviews with 0, \n",
    "#and cut the larger reviews to a max length \n",
    "#(we clip from the top, as the end of the reviews generally have a conclusion which provides better features)\n",
    "x_train = sequence.pad_sequences(token_tr_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = sequence.pad_sequences(token_te_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_val = sequence.pad_sequences(token_val_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "#changes the labels to one-hot encoding\n",
    "y_train = np_utils.to_categorical(tr_Y)\n",
    "y_test = np_utils.to_categorical(te_Y)\n",
    "y_val = np_utils.to_categorical(val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20000, 250)\n",
      "X_test shape: (25000, 250)\n",
      "X_val shape: (5000, 250)\n",
      "y_train shape: (20000, 2)\n",
      "y_test shape: (25000, 2)\n",
      "y_val shape: (5000, 2)\n",
      "*****************************************\n",
      "Tokenized Reviews Train sets and their labels\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.  1.]\n",
      "\n",
      "[ 9499  2518     6    48     6    29    42    14    10   301   952   450\n",
      "    29 16742   233    21    26     1  1950  6274]\n",
      "[ 0.  1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', x_train.shape)\n",
    "print('X_test shape:', x_test.shape)\n",
    "print('X_val shape:', x_val.shape)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "\n",
    "\n",
    "print(\"*****************************************\")\n",
    "print(\"Tokenized Reviews Train sets and their labels\")\n",
    "print(x_train[0][:20])\n",
    "print(y_train[0])\n",
    "print()\n",
    "print(x_train[1][:20])\n",
    "print(y_train[1])\n",
    "print()\n",
    "print(x_train[2][:20])\n",
    "print(y_train[2])\n",
    "print()\n",
    "print(x_train[3][:20])\n",
    "print(y_train[3])\n",
    "print()\n",
    "print(x_train[4][:20])\n",
    "print(y_train[4])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************************\n",
    "\n",
    "As you can see the reviews have now been transformed into indices to tokenized vocabulary and the labels have been converted to one-hot encoding. We can now go ahead and feed these sequences to Neural Network Models.\n",
    "\n",
    "********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A\n",
    "\n",
    "Building your first model (5 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 9,600,602\n",
      "Trainable params: 9,600,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_len, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200,activation=\"relu\"))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.4097 - acc: 0.7951 - val_loss: 0.2984 - val_acc: 0.8748\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.0479 - acc: 0.9854 - val_loss: 0.4392 - val_acc: 0.8532\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.0026 - acc: 0.9996 - val_loss: 0.5540 - val_acc: 0.8660\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 9.4813e-05 - acc: 1.0000 - val_loss: 0.5664 - val_acc: 0.8682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb1aa3a9978>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B\n",
    "\n",
    "Stacking Fully Connected Layers (5 points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 9,640,802\n",
      "Trainable params: 9,640,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_len, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200,activation=\"relu\"))\n",
    "model.add(Dense(200,activation=\"relu\"))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 59s 3ms/step - loss: 0.3997 - acc: 0.8044 - val_loss: 0.2950 - val_acc: 0.8758\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 59s 3ms/step - loss: 0.0425 - acc: 0.9861 - val_loss: 0.5775 - val_acc: 0.8476\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.0102 - acc: 0.9965 - val_loss: 0.6674 - val_acc: 0.8414\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.0115 - acc: 0.9958 - val_loss: 0.7365 - val_acc: 0.8520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8220b22828>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C\n",
    "\n",
    "Using LSTMS based networks(5 Points) \n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 3,348,354\n",
      "Trainable params: 3,348,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_len, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 170s 9ms/step - loss: 0.4442 - acc: 0.7874 - val_loss: 0.3414 - val_acc: 0.8622\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 169s 8ms/step - loss: 0.2228 - acc: 0.9147 - val_loss: 0.4139 - val_acc: 0.8150\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 169s 8ms/step - loss: 0.1367 - acc: 0.9495 - val_loss: 0.4158 - val_acc: 0.8608\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 170s 9ms/step - loss: 0.0812 - acc: 0.9725 - val_loss: 0.4831 - val_acc: 0.8570\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 169s 8ms/step - loss: 0.0483 - acc: 0.9845 - val_loss: 0.5379 - val_acc: 0.8538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81e8d1ee80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D\n",
    "\n",
    "Adding Pretrained Word Embeddings(10 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124252 unique tokens\n",
      "G Word embeddings: 1917494\n",
      "G Null word embeddings: 35772\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "#dimension of Glove Embeddings.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "#load glove embeddings\n",
    "gembeddings_index = {}\n",
    "with codecs.open('glove.42B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "#\n",
    "f.close()\n",
    "print('G Word embeddings:', len(gembeddings_index))\n",
    "\n",
    "# nb_words contains the total length of vocab\n",
    "nb_words = len(word_index) +1\n",
    "\n",
    "#get glove embeddings for each word in tokenizer.\n",
    "#g_word_embedding_matrix holds the embeddings dictionary\n",
    "g_word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    gembedding_vector = gembeddings_index.get(word)\n",
    "    if gembedding_vector is not None:\n",
    "        g_word_embedding_matrix[i] = gembedding_vector\n",
    "        \n",
    "#total words in the tokenizer not in Embedding matrix\n",
    "print('G Null word embeddings: %d' % np.sum(np.sum(g_word_embedding_matrix, axis=1) == 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 37,512,318\n",
      "Trainable params: 236,418\n",
      "Non-trainable params: 37,275,900\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM,\n",
    "                            weights=[g_word_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False))\n",
    "model.add(LSTM(128,recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "\n",
    "## to use the glove embeddings, your embedding layer would take the vocab size as input dimension, \n",
    "## Glove embedding dimension as the output dimsion\n",
    "## and you will provide the  embedding dictionary as the 'weights' parameter (!important) to the embedding layer.\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 216s 11ms/step - loss: 0.5227 - acc: 0.7303 - val_loss: 0.3382 - val_acc: 0.8586\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 216s 11ms/step - loss: 0.3123 - acc: 0.8690 - val_loss: 0.2811 - val_acc: 0.8836\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 216s 11ms/step - loss: 0.2570 - acc: 0.8953 - val_loss: 0.2652 - val_acc: 0.8918\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 216s 11ms/step - loss: 0.2241 - acc: 0.9114 - val_loss: 0.2569 - val_acc: 0.8942\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 216s 11ms/step - loss: 0.1894 - acc: 0.9283 - val_loss: 0.2609 - val_acc: 0.8952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8168b31358>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dont attempt this\n",
    "\n",
    "Stacking LSTM layers\n",
    "\n",
    "Unfortunately it takes very long to train, be aware we can stack LTMSs over each other like this.\n",
    "This requires bottom LSTM to return a sequences instead instead of single vector, which becomes input for the top LSTM.\n",
    "\n",
    "\n",
    "![title](img/model5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E\n",
    "\n",
    "Using Convolutional Networks (10 points)\n",
    "\n",
    "Construct the model, shown below. Use the same loss functions and optimizers as before\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model6.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 248, 128)          115328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               1999104   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 39,421,662\n",
      "Trainable params: 2,145,762\n",
      "Non-trainable params: 37,275,900\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[g_word_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False))\n",
    "\n",
    "model.add(Convolution1D(128, 3,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution1D(64, 3,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution1D(32, 3,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 82s 4ms/step - loss: 0.5042 - acc: 0.7239 - val_loss: 0.3131 - val_acc: 0.8722\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 83s 4ms/step - loss: 0.3132 - acc: 0.8692 - val_loss: 0.3289 - val_acc: 0.8610\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 82s 4ms/step - loss: 0.2761 - acc: 0.8860 - val_loss: 0.3593 - val_acc: 0.8348\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 82s 4ms/step - loss: 0.2420 - acc: 0.9013 - val_loss: 0.2871 - val_acc: 0.8834\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 83s 4ms/step - loss: 0.2022 - acc: 0.9195 - val_loss: 0.2823 - val_acc: 0.8870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f816a12ea20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F\n",
    "\n",
    "Model constructed : (5 points)\n",
    "\n",
    "Test Accuracy Over 87.5%: (5 Points)\n",
    "\n",
    "Bonus: Min(10, Square of (test_score - 88%))\n",
    "\n",
    "Create your best model, use Validation score to judge your best model and check accuracy on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 37,512,318\n",
      "Trainable params: 236,418\n",
      "Non-trainable params: 37,275,900\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[g_word_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128,recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep saving models with different names in model_name, \n",
    "\n",
    "so you can retrieve their weights again for testing, you dont have to retrain \n",
    "(You would have to initialize the model definition again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/7\n",
      "20000/20000 [==============================] - 209s 10ms/step - loss: 0.4586 - acc: 0.7810 - val_loss: 0.3111 - val_acc: 0.8656\n",
      "Epoch 2/7\n",
      "20000/20000 [==============================] - 207s 10ms/step - loss: 0.2910 - acc: 0.8789 - val_loss: 0.2826 - val_acc: 0.8812\n",
      "Epoch 3/7\n",
      "20000/20000 [==============================] - 206s 10ms/step - loss: 0.2510 - acc: 0.8985 - val_loss: 0.2800 - val_acc: 0.8900\n",
      "Epoch 4/7\n",
      "20000/20000 [==============================] - 207s 10ms/step - loss: 0.2181 - acc: 0.9123 - val_loss: 0.2601 - val_acc: 0.8946\n",
      "Epoch 5/7\n",
      "20000/20000 [==============================] - 207s 10ms/step - loss: 0.1827 - acc: 0.9287 - val_loss: 0.2681 - val_acc: 0.8902\n",
      "Epoch 6/7\n",
      "20000/20000 [==============================] - 207s 10ms/step - loss: 0.1437 - acc: 0.9444 - val_loss: 0.2882 - val_acc: 0.8988\n",
      "Epoch 7/7\n",
      "20000/20000 [==============================] - 206s 10ms/step - loss: 0.1072 - acc: 0.9607 - val_loss: 0.3212 - val_acc: 0.8846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb12def7e48>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_dir = \"./weights/\"\n",
    "model_name = 'model_best'\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "bst_model_path = wt_dir + model_name + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=7,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True,\n",
    "         callbacks=[early_stopping, model_checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan on using Ensemble averaging, feel free to edit the code below or add multiple models.\n",
    "\n",
    "Make sure they get saved and can be retrieved when executing serially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 79s 3ms/step\n",
      "Accuracy: 89.61%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(bst_model_path)\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part G\n",
    "\n",
    "Explain how Dense, LSTM and Convolution Layers work.\n",
    "\n",
    "Explain Relu, Dropout, and Softmax work.\n",
    "\n",
    "Analyze the architectures you constructed, with the accuracies you achieved and the training time it took. \n",
    "\n",
    "What are some insights you gained with these experiments? \n",
    "\n",
    "(5 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dense:\n",
    "     fully connected layers. Then every neuron in previous layer is input for every neuron in next layer. \n",
    "LSTM:\n",
    "    LSTM is a kind of RNN. Memorize long term information and important information previous stages. \n",
    "Convolution Layers:\n",
    "    Convolute input and pass output to the next layer.\n",
    "Relu:\n",
    "    relu(x) =   x if x > 0\n",
    "              { \n",
    "                0 if x <= 0\n",
    "    f(x)=max(0,x)\n",
    "Dropout:\n",
    "    Eliminate some nodes in neural network by some probability to avoid overfitting.\n",
    "Softmax work:\n",
    "    Given the prediction and calculate the probabilities for each class, f(x)=exp(x)/sum(exp(x))\n",
    "Analysis:\n",
    "    The loss reduced to 0.1072 and the training accuracy is 0.9607, validation loss is 0.3212 and accuracy is 0.8846. Then the testing accuracy is 89.8%.\n",
    "Insights:\n",
    "    LSTM keeps long-term information well. Dropout works well for avoiding overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
